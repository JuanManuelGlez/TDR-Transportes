# -*- coding: utf-8 -*-
"""EQUIPO 1_avance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t-c7uPEvdg3M7pNzMOpZknRzN_UtZRUG

# **Evidencia 2. Análisis de Aprendizaje Automático**
# Equipo 1

(20 puntos) Documenta el análisis exploratorio de los datos e incluye, los objetivos de la modelación y evaluación de los datos.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# Cargar los datos
data_path = '/content/Credit_Score.csv'
data = pd.read_csv(data_path)

# Ver las primeras filas del conjunto de datos
print(data.head())

# Verificar los tipos de datos de las columnas
print(data.dtypes)

# Resumen estadístico de los datos numéricos
print(data.describe())

# Revisar la información faltante
print(data.isnull().sum())

# Mapear los valores numéricos a etiquetas descriptivas
score_mapping = {0: 'Deficiente', 1: 'Bueno', 2: 'Estándar'}
data['credit_score'] = data['credit_score'].map(score_mapping)

#Distribución de la variable objetivo 'credit_score'
plt.figure(figsize=(8, 4))
sns.countplot(x='credit_score', data=data)
plt.title('Distribución de la puntuación crediticia')
plt.show()

"""El archivo muestra una serie de datos que reflejan diferentes aspectos del perfil financiero de los clientes, incluyendo su comportamiento de pago y características económicas. El objetivo de este análisis es entender la estructura del conjunto de datos para poder desarrollar un modelo de clasificación eficiente.


## Objetivos de la Modelación y Evaluación

Objetivo de la Modelación:
Buscamos desarrollar un sistema que clasifique automáticamente las puntuaciones crediticias de los clientes en tres categorías: "Deficiente", "Bueno" y "Estándar". Esto permitirá una asignación de recursos más eficiente y una mejor gestión del riesgo crediticio.

Evaluación del Modelo:
El modelo será evaluado utilizando métricas estándar en clasificación, incluyendo:

* Matriz de Confusión: Para visualizar el rendimiento del modelo en cada clase.
* Exactitud (Accuracy): Proporción de predicciones correctas.
* Precisión, Recall y F1-Score: Estas métricas ayudarán a balancear la precisión y la sensibilidad del modelo.

# Implementación de PCA

**Separación de datos**
"""

data=data.dropna()

data.head()

y = data['credit_score'] # clases
x=data.drop('credit_score',axis=1)
print(y.shape)
print(x.shape)

y

x

"""**Escalamiento de datos**"""

from sklearn.preprocessing import StandardScaler
# distribucion resultante queda con varianza igual a la unidad
sc = StandardScaler()
X_sc = sc.fit_transform(x)

"""**Visualización de variación explicada**"""

from sklearn.decomposition import PCA

pca = PCA()
X_pca = pca.fit_transform(X_sc)
print(X_pca.shape)

pca.explained_variance_ratio_

np.cumsum(pca.explained_variance_ratio_)

"""**Reducción de variables**"""

from sklearn.model_selection import train_test_split
pca = PCA(n_components=0.9)
X_pca=pca.fit_transform(X_sc)
X_pca.shape

X_train, X_test, y_train, y_test = train_test_split(
    X_pca, y, test_size=0.2, stratify=y,random_state=42)
X_train.shape

"""# Modelos supervisados

**Método del centroide**
"""

from sklearn.neighbors import NearestCentroid
md = NearestCentroid() # 1 Creamos el modelo por default

#  2 Entrenamos
md.fit(X_train, y_train)

y_pred = md.predict(X_test) # 3 Predicciones
print("Test set predictions using Min. Dist: ",format(y_pred))

from sklearn.metrics import accuracy_score # print the accuracy of the trained model

accuracy_score(y_test, y_pred)

"""**Método del clasificador**"""

from sklearn.neighbors import KNeighborsClassifier # Clasificador Scikit Learn
knn = KNeighborsClassifier(n_neighbors=3) # Hiperparametro k = 3

# 2 Entrenamiento
knn.fit(X_train, y_train)
# 3 Generar predicciones
y_pred = knn.predict(X_test)

print("Test set predictions using KNN: ",format(y_pred))

accuracy_score(y_test, y_pred)

"""**Método del arbol**"""

from sklearn.tree import DecisionTreeClassifier # Importamos
dt = DecisionTreeClassifier() # 1.- Creamos el modelo, sin restricciones en el árbol

dt.fit(X_train, y_train) # 2.- Entrenamos

# 3.- Predicciones
y_test_pred = dt.predict(X_test)
y_train_pred = dt.predict(X_train)

test_acc = accuracy_score(y_test_pred, y_test)
train_acc = accuracy_score(y_train_pred, y_train)
print('% de aciertos sobre el set de prueba: ', test_acc)
print('% de aciertos sobre el set de entrenamiento: ', train_acc)

from sklearn.pipeline import make_pipeline
from sklearn.svm import SVC # Support Vector Classifier

std_scl = StandardScaler()
svc = SVC(kernel='linear', C=100) # tipo de kernel, C: [Soft...C...Hard]

svc_clf = make_pipeline(std_scl, svc) # defino la secuencia de izq a der

#from sklearn.metrics import accuracy_score

#svc_clf.fit(X_train, y_train) # Entrenamos

# Predicciones
#y_test_pred = svc_clf.predict(X_test)
#y_train_pred = svc_clf.predict(X_train)

#test_acc = accuracy_score(y_test_pred, y_test)
#train_acc = accuracy_score(y_train_pred, y_train)
#print('% de aciertos sobre el set de PRUEBA: ', round(test_acc*100, 3))
#print('% de aciertos sobre el set de ENTRENAMIENTO: ', round(train_acc*100, 3))

"""# Modelos no supervisados"""

data

data.info()

# credit score int64
credit_score_mapping = {'Deficiente': 0, 'Bueno': 1, 'Estándar': 2}
data['credit_score'] = data['credit_score'].map(credit_score_mapping)

data.info()

# Seleccionar columnas numéricas para el clustering
data_numeric = data.select_dtypes(include=[np.number])
data_numeric

data_numeric[:10]

import pandas as pd
data_numeric = data_numeric.dropna()
data_numeric = data_numeric.fillna(data_numeric.mean())

import matplotlib.pyplot as plt
sns.scatterplot(data=data_numeric)
plt.show()

from sklearn.cluster import KMeans # importamos KMeans
#Inercia
km1 = KMeans(n_clusters = 3, init='k-means++') # Crear el modelo
km1.fit(data_numeric) # ajuste sus parametros (aprendizaje) con respecto a X
print(km1.inertia_) # Sum of squared distances to the closest centroid
# Mas pequeño el valor, mas cercanos entre si --> clusters mas compactos

# Elbow method: para evaluar el k óptimo
inertias = []
for i in range(1, 21):
  km = KMeans(n_clusters = i, init='k-means++')
  km.fit(data_numeric)
  inertias.append(km.inertia_)

# Grafica de las inercias: suma de distancias al cuadrado
plt.plot(range(1,21), inertias)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertias')
plt.axvline(3, color='k', linestyle='--')
plt.grid(axis='x', linestyle='--')
plt.show()

# Creamos el K-means para los 3 clusters encontrados
km = KMeans(n_clusters=3, init='k-means++') # kmeans++: acelera la convergencia
y_kmeans = km.fit_predict(data_numeric)
y_kmeans

km.cluster_centers_

import matplotlib.pyplot as plt
# Visualización de los clusters
plt.scatter(data_numeric.iloc[:, 0], data_numeric.iloc[:, 1], c=y_kmeans, s=50, cmap='viridis')

# Clusters of coustomers
plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], c='yellow', label='Centroids', s=100, alpha=0.5)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Clusters of data')
plt.legend()
plt.show()

silhouettes = []
for i in range(2, 11):
    km = KMeans(n_clusters = i, init='k-means++')
    km.fit(data_numeric)
    silhouettes.append(silhouette_score(data_numeric, km.labels_))
plt.plot(range(2,11), silhouettes, marker='o')
plt.title('Silhouette scores')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette score')
plt.axvline(4, color='k', linestyle='--')
plt.axvline(2, color='k', linestyle='--')
plt.grid(axis='x', linestyle='--')
plt.show()

"""Solucion: k=5 porque la inercia es mejor (menor) y score silhouette es "bueno".:"""

# Creamos el K-means para los 5 clusters encontrados
km = KMeans(n_clusters=5, init='k-means++') # kmeans++: acelera la convergencia
y_kmeans = km.fit_predict(data_numeric)
y_kmeans

import matplotlib.pyplot as plt
# Visualización de los clusters
plt.scatter(data_numeric.iloc[:, 0], data_numeric.iloc[:, 1], c=y_kmeans, s=50, cmap='viridis')

# Clusters of coustomers
plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], c='yellow', label='Centroids', s=100, alpha=0.5)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Clusters of data')
plt.legend()
plt.show()